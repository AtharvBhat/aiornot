{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1986f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678e21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "epochs = 5\n",
    "# model = torchvision.models.convnext_large(weights=\"DEFAULT\", progress=True)\n",
    "# model.classifier[2] = nn.Linear(1536, 2)\n",
    "model = torchvision.models.convnext_tiny(weights=\"DEFAULT\", progress=True)\n",
    "model.classifier[2] = nn.Linear(768, 2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7621cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = torchvision.transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.Resize((224,224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "full_dataset = utils.train_dataset(\"../dataset/train.csv\", tf=tf)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, num_workers = 4)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size = batch_size, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f51fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    model.train()\n",
    "    print(f\"Running Epoch {epoch}\")\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_dataloader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "        predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "        correct = (predictions == target).sum().item()\n",
    "        accuracy = correct / batch_size\n",
    "        accuracies.append(accuracy)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        wandb.log({\"step loss\" : loss.item()})\n",
    "    \n",
    "    accuracy = np.array(accuracies).mean()\n",
    "    loss = np.array(losses).mean()\n",
    "    wandb.log({\"Avg Epoch Train Loss\" : loss, \"Epoch Train Accuracy\" : accuracy})\n",
    "    print(f\"Epoch {epoch} : Avg Train accuracy : {accuracy}, Avg Loss : {loss}\")\n",
    "    return accuracy, loss\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in validation_dataloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        validation_loss += F.cross_entropy(output, target, reduction=\"sum\").item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print('Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    wandb.log({\"Validation Loss\" : validation_loss, \"Validation Accuracy\" : correct / len(val_loader.dataset)})\n",
    "    return (correct / len(val_loader.dataset)), validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f99b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matharvbhat\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/atharvbhat/aiornot-notebooks/runs/1px0pvio\" target=\"_blank\">fortuitous-ox-11</a></strong> to <a href=\"https://wandb.ai/atharvbhat/aiornot-notebooks\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▎                                                                          | 36/466 [00:38<05:25,  1.32it/s]"
     ]
    }
   ],
   "source": [
    "wandb.init()\n",
    "model.to(device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    acc, loss = train(epoch)\n",
    "    train_accuracy.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "    acc, loss = validation()\n",
    "    scheduler.step(loss)\n",
    "    torch.save(model.state_dict(), f\"../model/{epoch}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
